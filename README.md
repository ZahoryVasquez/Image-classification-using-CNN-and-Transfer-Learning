# ğŸ–¼ï¸ Image Classification on CIFAR-10: From Scratch to Transfer Learning

**How far can we push accuracy on CIFAR-10 by evolving from a simple CNN to cutting-edge pretrained models?**

---

## ğŸ“‹ Project Overview

This project explores the performance evolution from custom-built CNNs to modern transfer learning models on the CIFAR-10 dataset. Unlike many projects that stop at a single CNN, this work systematically compares multiple architectural improvements and regularization techniques before benchmarking against pretrained models.

---

## ğŸ¯ Research Question

**What happens when we compare custom-built CNNs against modern transfer learning models on CIFAR-10?**

We answer this by exploring a step-by-step evolution of improvements:

1. Baseline CNN (from scratch) â€” a simple ConvNet to establish a starting point
2. Deeper CNN â€” added more convolutional layers and dropout to extract richer features
3. BatchNorm, Dropout & Regularization â€” introduced Batch Normalization, GlobalAveragePooling and L2 weight decay
4. Early Stopping â€” tuned training with early stopping to avoid overfitting
5. Data Augmentation â€” applied random rotations, flips and shifts to improve robustness
6. Transfer Learning (MobileNetV2 variants) â€” benchmarked several MobileNetV2 head/fine-tuning strategies

---

## ğŸ“Š Dataset

**Dataset:** CIFAR-10  
**Size:** 60,000 color images (32x32 pixels) across 10 balanced classes:  
âœˆï¸ airplane | ğŸš— automobile | ğŸ¦ bird | ğŸ± cat | ğŸ¦Œ deer | ğŸ¶ dog | ğŸ¸ frog | ğŸ´ horse | ğŸš¢ ship | ğŸšš truck  

**Split:** 50,000 training images + 10,000 testing images  

**Challenges:**
- Low-resolution images â†’ harder to extract strong features
- Some categories are very similar (cat vs dog, car vs truck)

---

## ğŸ› ï¸ Methodology & Models

### ğŸ”¹ Custom CNNs

| Model | Architecture | Test Accuracy | Training Accuracy |
|-------|-------------|---------------|-------------------|
| **Baseline CNN** | Conv2D(32) â†’ MaxPool â†’ Conv2D(64) â†’ MaxPool â†’ Dense(128) â†’ Softmax | ~68.9% | ~84% |
| **Deeper CNN + Dropout** | Added Conv2D(128), Dense(256), Dropout(0.5) | 73.4% | 85% |
| **CNN + BatchNorm + GAP** | Added BatchNormalization + GlobalAveragePooling | 73.1% | 91% |
| **CNN + Regularizer (L2)** | Added weight decay regularization | 72.5% | 93% |
| **CNN + Early Stopping** | Hyperparameter tuning + early stopping | 72.9% | 94% |
| **CNN + Data Augmentation** | Rotations, flips, shifts | **76.9%** âœ… | 74% |

### ğŸ”¹ Transfer Learning

| Model | Architecture | Test Accuracy |
|-------|-------------|---------------|
| **MobileNetV2.1** | MobileNetV2 â†’ GAP â†’ Dense(128, ReLU) â†’ Dense(10, Softmax) | **86.6%** âœ… |
| **MobileNetV2.2** | MobileNetV2 â†’ GAP â†’ Dropout(0.3) â†’ Dense(10, Softmax) | 85.6% |
| **MobileNetV2.3** | Same as V2.2, batch size 64 | 85.6% |

---

## ğŸ“ˆ Key Insights

- ğŸ“ˆ Custom CNNs improved step by step, but plateaued around ~77%
- ğŸ§  Transfer learning (MobileNetV2) significantly outperformed scratch CNNs
- ğŸ” MobileNetV2.1 achieved state-of-the-art performance on CIFAR-10 at 86.6%
- ğŸ±ğŸ¶ Most misclassifications: cats vs dogs, cars vs trucks

---

ğŸ™‹ğŸ½â€â™€ï¸ **About Me**

- **RocÃ­o Zahory VÃ¡squez Romero**  
- Senior Auditor | Data Science & Machine Learning Enthusiast  
- Email: rocio.vasquez@usach.cl  
- LinkedIn: [https://www.linkedin.com/in/rocio-zahory-vasquez-romero-3621ab1a7/](https://www.linkedin.com/in/rocio-zahory-vasquez-romero-3621ab1a7/)

---
âš¡ **How to Run**

1. Clone the repo:  
```bash
git clone https://github.com/nataliadominguez99/Image-classification-using-CNN-and-Transfer-Learning.git
cd Image-classification-using-CNN-and-Transfer-Learning

2.Install dependencies:
pip install -r requirements.txt

3. Run the main notebook: 
jupyter notebook "ProjectCifar.ipynb"


